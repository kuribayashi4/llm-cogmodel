{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "model_order = [\n",
    "    \"Llama-2-7b-chat-hf\", \n",
    "    \"Llama-2-13b-chat-hf\", \n",
    "    \"Llama-2-70b-chat-hf\", \n",
    "    \"falcon-7b-instruct\",\n",
    "    \"falcon-40b-instruct\",\n",
    "    \"text-davinci-002\",\n",
    "    \"text-davinci-003\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/NS/all.txt.annotation.filtered.averaged_rt.csv\")\n",
    "article2sents = defaultdict(list)\n",
    "for article_id, article in groupby(df[[\"word\", \"time\", \"sent_id\", \"article\"]].iterrows(), key=lambda x: x[1][3]):\n",
    "    for sent_id, sent in groupby(article, key=lambda x: x[1][2]):\n",
    "        sent = list(sent)\n",
    "        sent = {f\"{str(i)}: {token[1][0]}\": token[1][1] for i, token in enumerate(sent)}\n",
    "        article2sents[str(article_id)].append(sent)\n",
    "    # sent = list(sent)\n",
    "    # sents.append([(\"\".join(tok[1][0].split()).replace(\"‚ñÅ\", \" \").strip(), tok[1][1]) for tok in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/NS/*/prompt_estimation/prompt_*.json', recursive=True)\n",
    "files = [file for file in files if \"direct\" not in file and \"surprisal\" not in file]\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    results = json.load(open(file))\n",
    "    correls = []\n",
    "    for article_id, preds in results.items():\n",
    "        golds = article2sents[article_id]\n",
    "        # assert len(golds) == len(preds)\n",
    "        preds = preds[:5]\n",
    "        for pred, gold in zip(preds, golds):\n",
    "            # print(pred)\n",
    "            target = pred.split(\"\\n\\n\")[3]\n",
    "            if len(target.split(\"\\n\")) < 6:\n",
    "                continue\n",
    "            token_ids = [token for token in target.split(\"\\n\")[3].rstrip(\",\").split(\", \")]\n",
    "            answer = [token for token in target.split(\"\\n\")[5].split(\", \")]\n",
    "            answer_valid = [a for a in answer if a in token_ids]\n",
    "            if len(answer_valid) > 1:\n",
    "                y = [-gold[ans] for ans in answer_valid]\n",
    "                x = list(range(len(y)))\n",
    "                correl = spearmanr(x,y)[0]\n",
    "                if not math.isnan(correl):\n",
    "                    correls.append(correl)\n",
    "    with open(file+\".correl\", \"w\") as f:\n",
    "        f.write(str(mean(correls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/NS/*/prompt_estimation/prompt_*.json.correl', recursive=True)\n",
    "files = [file for file in files if \"direct\" not in file and \"surprisal\" not in file]\n",
    "\n",
    "df_results1 = pd.DataFrame(columns=[\"model\", \"prompt\", \"correl\"])\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        print(file)\n",
    "        model = file.split(\"/\")[-3]\n",
    "        prompt = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        correl = float(f.read().strip())\n",
    "        df_results1.loc[len(df_results1)] = [model, prompt, float(correl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_ns = pd.DataFrame(columns=[\"model\", \"result\"])\n",
    "df_result_ns[\"result\"] = df_results1.groupby(\"model\")[[\"correl\"]].mean()[\"correl\"].round(2).apply(str)\n",
    "df_result_ns[\"result\"] = df_result_ns[\"result\"] + \" $\\\\pm$ \" + df_results1.groupby(\"model\")[[\"correl\"]].std()[\"correl\"].round(2).apply(str)\n",
    "print(df_result_ns.reindex(model_order)[\"result\"].to_latex().replace(\"\\\\\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/NS/*/prompt_estimation/prompt_surprisal*.json', recursive=True)\n",
    "files = [file for file in files if \"direct\" not in file]\n",
    "\n",
    "for file in files:\n",
    "    article2sups = defaultdict(list)\n",
    "    model = file.split(\"/\")[3]\n",
    "    target_file = f\"../results/NS/{model}/surprisal.json\"\n",
    "    article2surprisals = json.load(open(target_file))\n",
    "    all_sups = [sup for _, sents_sups in sorted(article2surprisals.items(), key=lambda x: int(x[0])) for sent_sups in sents_sups for sup in sent_sups]\n",
    "    # assert len(df) == len(all_sups)\n",
    "    df[\"surprisal\"] = all_sups\n",
    "    for article_id, article in groupby(df[[\"word\", \"time\", \"surprisal\", \"sent_id\", \"article\"]].iterrows(), key=lambda x: x[1][4]):\n",
    "        for sent_id, sent in groupby(article, key=lambda x: x[1][3]):\n",
    "            sent = list(sent)\n",
    "            sent = {f\"{str(i)}: {token[1][0]}\": token[1][2] for i, token in enumerate(sent)}\n",
    "            article2sups[str(article_id)].append(sent)\n",
    "\n",
    "    correls = []\n",
    "    with open(file+\".correl\", \"w\") as f:\n",
    "        for article_id, sents in article2sents.items():\n",
    "            assert len(sents) == len(article2sups[article_id])\n",
    "            sents = sents[:5]\n",
    "            for sent, sups in zip(sents, article2sups[article_id]):\n",
    "                assert len(sent) == len(sups)\n",
    "                times = [time for tok, time in sent.items()]\n",
    "                sups = [sups[tok] for tok, _ in sent.items()]\n",
    "                correl = spearmanr(times, sups)[0]\n",
    "                if not math.isnan(correl):\n",
    "                    correls.append(correl)\n",
    "        surprisal_time_correl = mean(correls)\n",
    "        f.write(\"surpisal v.s. time: \" + str(surprisal_time_correl) + \"\\n\")\n",
    "\n",
    "        results = json.load(open(file))\n",
    "        correls = []\n",
    "        for article_id, preds in results.items():\n",
    "            golds = article2sups[article_id]\n",
    "            preds = preds[:5]\n",
    "            # assert len(golds) == len(preds)\n",
    "            for pred, gold in zip(preds, golds):\n",
    "                # print(pred)\n",
    "                target = pred.split(\"\\n\\n\")[3]\n",
    "                if len(target.split(\"\\n\")) < 6:\n",
    "                    continue\n",
    "                token_ids = [token for token in target.split(\"\\n\")[3].rstrip(\",\").split(\", \")]\n",
    "                answer = [token for token in target.split(\"\\n\")[5].split(\", \")]\n",
    "                answer_valid = [a for a in answer if a in token_ids]\n",
    "                if len(answer_valid) > 1:\n",
    "                    y = [-gold[ans] for ans in answer_valid]\n",
    "                    x = list(range(len(y)))\n",
    "                    correl = spearmanr(x,y)[0]\n",
    "                    if not math.isnan(correl):\n",
    "                        correls.append(correl)\n",
    "        f.write(\"response v.s. surprisal: \" + str(mean(correls)) + \"\\n\")\n",
    "        \n",
    "        correls = []\n",
    "        for article_id, preds in results.items():\n",
    "            golds = article2sents[article_id]\n",
    "            preds = preds[:5]\n",
    "            # assert len(golds) == len(preds)\n",
    "            for pred, gold in zip(preds, golds):\n",
    "                # print(pred)\n",
    "                target = pred.split(\"\\n\\n\")[3]\n",
    "                if len(target.split(\"\\n\")) < 6:\n",
    "                    continue\n",
    "                token_ids = [token for token in target.split(\"\\n\")[3].rstrip(\",\").split(\", \")]\n",
    "                answer = [token for token in target.split(\"\\n\")[5].split(\", \")]\n",
    "                answer_valid = [a for a in answer if a in token_ids]\n",
    "                if len(answer_valid) > 1:\n",
    "                    y = [-gold[ans] for ans in answer_valid]\n",
    "                    x = list(range(len(y)))\n",
    "                    correl = spearmanr(x,y)[0]\n",
    "                    if not math.isnan(correl):\n",
    "                        correls.append(correl)\n",
    "        f.write(\"response v.s. time: \" + str(mean(correls)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/NS/*/prompt_estimation/prompt_surprisal*.json.correl', recursive=True)\n",
    "df_results = pd.DataFrame(columns=[\"model\", \"prompt\", \"metalinguistic\", \"surprisal\", \"metacognition\"])\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        print(file)\n",
    "        model = file.split(\"/\")[-3]\n",
    "        prompt = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        data = \"\\n\".join(f.readlines())\n",
    "        correl_surprisal = float(data.split(\"\\n\")[0].split(\": \")[1])\n",
    "        correl_self_surprisal = float(data.split(\"\\n\")[2].split(\": \")[1])\n",
    "        correl_prompt_surprisal = float(data.split(\"\\n\")[4].split(\": \")[1])\n",
    "        df_results.loc[len(df_results)] = [model, prompt, correl_prompt_surprisal, correl_surprisal, correl_self_surprisal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_ns = pd.DataFrame(columns=[\"model\", \"metalinguistic\", \"surprisal\", \"metacognition\"])\n",
    "df_result_ns[\"metalinguistic\"] = df_results.groupby(\"model\")[[\"metalinguistic\"]].mean()[\"metalinguistic\"].round(2).apply(str)\n",
    "df_result_ns[\"metalinguistic\"] = df_result_ns[\"metalinguistic\"] + \" $\\\\pm$ \" + df_results.groupby(\"model\")[[\"metalinguistic\"]].std()[\"metalinguistic\"].round(2).apply(str)\n",
    "\n",
    "df_result_ns[\"surprisal\"] = df_results.groupby(\"model\")[[\"surprisal\"]].mean()[\"surprisal\"].round(2).apply(str)\n",
    "df_result_ns[\"surprisal\"] = df_result_ns[\"surprisal\"] + \" $\\\\pm$ \" + df_results.groupby(\"model\")[[\"surprisal\"]].std()[\"surprisal\"].round(2).apply(str)\n",
    "\n",
    "df_result_ns[\"metacognition\"] = df_results.groupby(\"model\")[[\"metacognition\"]].mean()[\"metacognition\"].round(2).apply(str)\n",
    "df_result_ns[\"metacognition\"] = df_result_ns[\"metacognition\"] + \" $\\\\pm$ \" + df_results.groupby(\"model\")[[\"metacognition\"]].std()[\"metacognition\"].round(2).apply(str)\n",
    "\n",
    "df_results = pd.merge(df_results, df_results1, on=\"model\", how=\"outer\")\n",
    "\n",
    "print(df_result_ns.reindex(model_order)[\"metalinguistic\"].to_latex().replace(\"\\\\\", \"\"))\n",
    "print(df_result_ns.reindex(model_order)[\"surprisal\"].to_latex().replace(\"\\\\\", \"\"))\n",
    "print(df_result_ns.reindex(model_order)[\"metacognition\"].to_latex().replace(\"\\\\\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(stats.mannwhitneyu(df_results[\"metalinguistic\"].values.tolist() + df_results[\"correl\"].values.tolist(), y=df_results[\"surprisal\"].values, alternative=\"two-sided\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "df = pd.read_csv(\"../data/DC/all.txt.annotation.filtered.averaged_rt.csv\")\n",
    "article2sents = defaultdict(list)\n",
    "for article_id, article in groupby(df[[\"surface\", \"time\", \"sent_id\", \"article\"]].iterrows(), key=lambda x: x[1][3]):\n",
    "    for sent_id, sent in groupby(article, key=lambda x: x[1][2]):\n",
    "        sent = list(sent)\n",
    "        sent = {f\"{str(i)}: {''.join(token[1][0].split()).replace('‚ñÅ', ' ').strip()}\": token[1][1] for i, token in enumerate(sent)}\n",
    "        article2sents[str(article_id)].append(sent)\n",
    "        # sent = list(sent)\n",
    "        # sents.append([(\"\".join(tok[1][0].split()).replace(\"‚ñÅ\", \" \").strip(), tok[1][1]) for tok in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "\n",
    "files = glob.glob('../results/DC/*/prompt_estimation/prompt_*.json', recursive=True)\n",
    "files = [file for file in files if \"direct\" not in file and \"surprisal\" not in file]\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    results = json.load(open(file))\n",
    "    correls = []\n",
    "    for article_id, preds in results.items():\n",
    "        golds = article2sents[article_id]\n",
    "        # assert len(golds) == len(preds)\n",
    "        preds = preds[:5]\n",
    "        for pred, gold in zip(preds, golds):\n",
    "            # print(pred)\n",
    "            target = pred.split(\"\\n\\n\")[3]\n",
    "            if len(target.split(\"\\n\")) < 6:\n",
    "                continue\n",
    "            token_ids = [token for token in target.split(\"\\n\")[3].rstrip(\",\").split(\", \")]\n",
    "            answer = [token for token in target.split(\"\\n\")[5].split(\", \")]\n",
    "            answer_valid = [a for a in answer if a in token_ids]\n",
    "            if len(answer_valid) > 1:\n",
    "                try:\n",
    "                    y = [-gold[ans] for ans in answer_valid]\n",
    "                    x = list(range(len(y)))\n",
    "                    correl = spearmanr(x,y)[0]\n",
    "                    if not math.isnan(correl):\n",
    "                        correls.append(correl)\n",
    "                except:\n",
    "                    pass\n",
    "    if correls:\n",
    "        with open(file+\".correl\", \"w\") as f:\n",
    "            f.write(str(mean(correls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/DC/*/prompt_estimation/prompt_*.json.correl', recursive=True)\n",
    "files = [file for file in files if \"direct\" not in file and \"surprisal\" not in file]\n",
    "\n",
    "df_results_1 = pd.DataFrame(columns=[\"model\", \"prompt\", \"correl\"])\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        print(file)\n",
    "        model = file.split(\"/\")[-3]\n",
    "        prompt = file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "        correl = float(f.read().strip())\n",
    "        df_results_1.loc[len(df_results_1)] = [model, prompt, float(correl)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_dc = pd.DataFrame(columns=[\"model\", \"result\"])\n",
    "df_result_dc[\"result\"] = df_results_1.groupby(\"model\")[[\"correl\"]].mean()[\"correl\"].round(2).apply(str)\n",
    "df_result_dc[\"result\"] = df_result_dc[\"result\"] + \" $\\\\pm$ \" + df_results_1.groupby(\"model\")[[\"correl\"]].std()[\"correl\"].round(2).apply(str)\n",
    "for line in df_result_dc.reindex(model_order)[\"result\"].to_latex().replace(\"\\\\\", \"\").split(\"\\n\"):\n",
    "    if \"&\" in line:\n",
    "        print(\" & \".join(line.split(\"&\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/DC/*/prompt_estimation/prompt_surprisal*.json', recursive=True)\n",
    "files = [file for file in files if \"direct\" not in file]\n",
    "\n",
    "for file in files:\n",
    "    article2sups = defaultdict(list)\n",
    "    model = file.split(\"/\")[3]\n",
    "    target_file = f\"../results/DC/{model}/surprisal.json\"\n",
    "    article2surprisals = json.load(open(target_file))\n",
    "    all_sups = [sup for article_id, sents_sups in sorted(article2surprisals.items(), key=lambda x: int(x[0])) for sent_sups in sents_sups for sup in sent_sups]\n",
    "    # assert len(df) == len(all_sups)\n",
    "    df[\"surprisal\"] = all_sups\n",
    "    for article_id, article in groupby(df[[\"surface\", \"time\", \"surprisal\", \"sent_id\", \"article\"]].iterrows(), key=lambda x: x[1][4]):\n",
    "        # if int(article_id) < 6:\n",
    "        for sent_id, sent in groupby(article, key=lambda x: x[1][3]):\n",
    "            sent = list(sent)\n",
    "            sent = {f\"{str(i)}: {''.join(token[1][0].split()).replace('‚ñÅ', ' ').strip()}\": token[1][2] for i, token in enumerate(sent)}\n",
    "            article2sups[str(article_id)].append(sent)\n",
    "\n",
    "    correls = []\n",
    "    with open(file+\".correl\", \"w\") as f:\n",
    "        for article_id, sents in article2sents.items():\n",
    "            # if int(article_id) < 6:\n",
    "            assert len(sents) == len(article2sups[article_id])\n",
    "            sents = sents[:5]\n",
    "            for sent, sups in zip(sents, article2sups[article_id]):\n",
    "                # assert len(sent) == len(sups)\n",
    "                times = [time for tok, time in sent.items()]\n",
    "                sups = [sups[tok] for tok, _ in sent.items()]\n",
    "                correl = spearmanr(times, sups)[0]\n",
    "                if not math.isnan(correl):\n",
    "                    correls.append(correl)\n",
    "        surprisal_time_correl = mean(correls)\n",
    "        \n",
    "        if correls:\n",
    "            f.write(\"surpisal v.s. time: \" + str(surprisal_time_correl) + \"\\n\")\n",
    "\n",
    "        results = json.load(open(file))\n",
    "        correls = []\n",
    "        for article_id, preds in results.items():\n",
    "            preds = preds[:5]\n",
    "            golds = article2sups[article_id]\n",
    "            # assert len(golds) == len(preds)\n",
    "            for pred, gold in zip(preds, golds):\n",
    "                # print(pred)\n",
    "                target = pred.split(\"\\n\\n\")[3]\n",
    "                if len(target.split(\"\\n\")) < 6:\n",
    "                    continue\n",
    "                token_ids = [token for token in target.split(\"\\n\")[3].rstrip(\",\").split(\", \")]\n",
    "                answer = [token for token in target.split(\"\\n\")[5].split(\", \")]\n",
    "                answer_valid = [a for a in answer if a in token_ids]\n",
    "                if len(answer_valid) > 1:\n",
    "                    try:\n",
    "                        y = [-gold[ans] for ans in answer_valid]\n",
    "                        x = list(range(len(y)))\n",
    "                        correl = spearmanr(x,y)[0]\n",
    "                        if not math.isnan(correl):\n",
    "                            correls.append(correl)\n",
    "                    except:\n",
    "                        pass\n",
    "        if correls:\n",
    "            f.write(\"response v.s. surprisal: \" + str(mean(correls)) + \"\\n\")\n",
    "        \n",
    "        correls = []\n",
    "        for article_id, preds in results.items():\n",
    "            golds = article2sents[article_id]\n",
    "            # assert len(golds) == len(preds)\n",
    "            preds = preds[:5]\n",
    "            for pred, gold in zip(preds, golds):\n",
    "                # print(pred)\n",
    "                target = pred.split(\"\\n\\n\")[3]\n",
    "                token_ids = [token for token in target.split(\"\\n\")[3].rstrip(\",\").split(\", \")]\n",
    "                answer = [token for token in target.split(\"\\n\")[5].split(\", \")]\n",
    "                answer_valid = [a for a in answer if a in token_ids]\n",
    "                if len(answer_valid) > 1:\n",
    "                    try:\n",
    "                        y = [-gold[ans] for ans in answer_valid]\n",
    "                        x = list(range(len(y)))\n",
    "                        correl = spearmanr(x,y)[0]\n",
    "                        if not math.isnan(correl):\n",
    "                            correls.append(correl)\n",
    "                    except:\n",
    "                        pass\n",
    "        if correls:\n",
    "            f.write(\"response v.s. time: \" + str(mean(correls)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../results/DC/*/prompt_estimation/prompt_surprisal*.json.correl', recursive=True)\n",
    "df_results = pd.DataFrame(columns=[\"model\", \"prompt\", \"metalinguistic\", \"surprisal\", \"metacognition\"])\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        print(file)\n",
    "        model = file.split(\"/\")[-3]\n",
    "        prompt = file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "        data = [line for line in f.readlines() if line.strip()]\n",
    "        if len(data) < 3:\n",
    "            continue\n",
    "        correl_surprisal = float(data[0].split(\": \")[1])\n",
    "        correl_self_surprisal = float(data[1].split(\": \")[1])\n",
    "        correl_prompt_surprisal = float(data[2].split(\": \")[1])\n",
    "        df_results.loc[len(df_results)] = [model, prompt, correl_prompt_surprisal, correl_surprisal, correl_self_surprisal]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_dc = pd.DataFrame(columns=[\"model\", \"metalinguistic\", \"surprisal\", \"metacognition\"])\n",
    "df_result_dc[\"metalinguistic\"] = df_results.groupby(\"model\")[[\"metalinguistic\"]].mean()[\"metalinguistic\"].round(2).apply(str)\n",
    "df_result_dc[\"metalinguistic\"] = df_result_dc[\"metalinguistic\"] + \" $\\\\pm$ \" + df_results.groupby(\"model\")[[\"metalinguistic\"]].std()[\"metalinguistic\"].round(2).apply(str)\n",
    "\n",
    "df_result_dc[\"surprisal\"] = df_results.groupby(\"model\")[[\"surprisal\"]].mean()[\"surprisal\"].round(2).apply(str)\n",
    "df_result_dc[\"surprisal\"] = df_result_dc[\"surprisal\"] + \" $\\\\pm$ \" + df_results.groupby(\"model\")[[\"surprisal\"]].std()[\"surprisal\"].round(2).apply(str)\n",
    "\n",
    "df_result_dc[\"metacognition\"] = df_results.groupby(\"model\")[[\"metacognition\"]].mean()[\"metacognition\"].round(2).apply(str)\n",
    "df_result_dc[\"metacognition\"] = df_result_dc[\"metacognition\"] + \" $\\\\pm$ \" + df_results.groupby(\"model\")[[\"metacognition\"]].std()[\"metacognition\"].round(2).apply(str)\n",
    "\n",
    "df_results = pd.merge(df_results, df_results_1)\n",
    "\n",
    "for line in df_result_dc.reindex(model_order)[\"metalinguistic\"].to_latex().replace(\"\\\\\", \"\").split(\"\\n\"):\n",
    "    if \"&\" in line:\n",
    "        print(\" & \".join(line.split(\"&\")[1:]))\n",
    "\n",
    "for line in df_result_dc.reindex(model_order)[\"surprisal\"].to_latex().replace(\"\\\\\", \"\").split(\"\\n\"):\n",
    "    if \"&\" in line:\n",
    "        print(\" & \".join(line.split(\"&\")[1:]))\n",
    "\n",
    "for line in df_result_dc.reindex(model_order)[\"metacognition\"].to_latex().replace(\"\\\\\", \"\").split(\"\\n\"):\n",
    "    if \"&\" in line:\n",
    "        print(\" & \".join(line.split(\"&\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(stats.mannwhitneyu(df_results[\"metalinguistic\"].values.tolist() + df_results[\"correl\"].values.tolist(), y=df_results[\"surprisal\"].values, alternative=\"two-sided\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
