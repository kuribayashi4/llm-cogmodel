{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordfreq import zipf_frequency, tokenize\n",
    "from statistics import mean\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_target_sent(text):\n",
    "    if \"Answer:\\n\" in text:\n",
    "        return text.split(\"Answer:\\n\")[1]\n",
    "    else:\n",
    "        return text.split(\"\\n\")[1]\n",
    "    \n",
    "def get_word_freqs(sents, exclude_stopwords=True):\n",
    "    freqs = []\n",
    "    for sent in sents:\n",
    "        if exclude_stopwords:\n",
    "            freqs.append([zipf_frequency(tok, \"en\", minimum=0.0) for tok in tokenize(\" \".join(get_target_sent(sent).split()[4:]), \"en\") if tok.lower() not in stop_words])\n",
    "        else:\n",
    "            freqs.append([zipf_frequency(tok, \"en\", minimum=0.0) for tok in tokenize(\" \".join(get_target_sent(sent).split()[4:], \"en\"))])\n",
    "    freqs = [f for freq in freqs for f in freq if f != 0]\n",
    "    return freqs\n",
    "\n",
    "def get_word_lens(sents):\n",
    "    word_lens = [len(tok) for sent in sents for tok in word_tokenize(\" \".join(get_target_sent(sent).split()[4:]))]\n",
    "    return word_lens\n",
    "\n",
    "def get_dep_lens(sents):\n",
    "    dep_lens = []\n",
    "    for text in sents:\n",
    "        for token in nlp(sent_tokenize(get_target_sent(text))[0]):\n",
    "            dep_lens.append(abs(token.i - token.head.i))\n",
    "    # dep_lens = [dep for dep in dep_lens if dep > 5]\n",
    "    return dep_lens\n",
    "\n",
    "def get_sent_lens(sents):\n",
    "    sent_lens = [len(word_tokenize(sent)) for text in sents for sent in sent_tokenize(get_target_sent(text))[:1]]\n",
    "    sent_lens = [sent_len for sent_len in sent_lens if sent_len > 3]\n",
    "    return sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2id = {\n",
    "    \"simple_grammar\": \"Syn$\\\\downarrow$\",\n",
    "    \"middle_grammar\": \"Syn$\\\\rightarrow$\",\n",
    "    \"difficult_grammar\": \"Syn$\\\\uparrow$\",\n",
    "    \"simple_vocab\": \"Lex$\\\\downarrow$\",\n",
    "    \"middle_vocab\": \"Lex$\\\\rightarrow$\",\n",
    "    \"difficult_vocab\": \"Lex$\\\\uparrow$\",\n",
    "    \"human_like_1\": \"Task1\",\n",
    "    \"human_like_2\": \"Task2\",\n",
    "    \"baseline\": \"baseline\",\n",
    "}\n",
    "model_orders = [\"Llama-2-7b-chat-hf\", \"Llama-2-13b-chat-hf\", \"Llama-2-70b-chat-hf\", \"falcon-7b-instruct\", \"falcon-40b-instruct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model\", \"prompt\", \"sents\"])\n",
    "\n",
    "row_list = []\n",
    "for dir in model_orders:\n",
    "    if \"Llama\" in dir:\n",
    "        baseline = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"baseline\", \"sents\": baseline})\n",
    "        simple_vocab = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence_using_the_simplest_vocabulary_possible.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"simple_vocab\", \"sents\": simple_vocab})\n",
    "        middle_vocab = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence_with_a_careful_focus_on_word_choice.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"middle_vocab\", \"sents\": middle_vocab})\n",
    "        difficult_vocab = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence_using_the_most_difficult_vocabulary_possible.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"difficult_vocab\", \"sents\": difficult_vocab})\n",
    "        simple_grammar = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_grammatically_simple_sentence_as_much_as_possible.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"simple_grammar\", \"sents\": simple_grammar})\n",
    "        middle_grammar = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence_with_a_careful_focus_on_grammar.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"middle_grammar\", \"sents\": middle_grammar})\n",
    "        difficult_grammar = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_grammatically_complex_sentence_as_much_as_possible.--Answer:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"difficult_grammar\", \"sents\": difficult_grammar})\n",
    "        human_like_1 = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence_in_a_human-like_manner._It_has_been_reported_that_human_ability_to_predic.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"human_like_1\", \"sents\": human_like_1})\n",
    "        human_like_2 = json.load(open(f\"../sampled/DC/{dir}/Please_generate_a_sentence._We_are_trying_to_reproduce_human_reading_times_with_the_word_prediction_.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"human_like_2\", \"sents\": human_like_2})\n",
    "    else:\n",
    "        baseline = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"baseline\", \"sents\": baseline})\n",
    "        simple_vocab = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_using_the_simplest_vocabulary_possible:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"simple_vocab\", \"sents\": simple_vocab})\n",
    "        middle_vocab = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_with_a_careful_focus_on_word_choice.-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"middle_vocab\", \"sents\": middle_vocab})\n",
    "        difficult_vocab = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_using_the_most_difficult_vocabulary_possible:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"difficult_vocab\", \"sents\": difficult_vocab})\n",
    "        simple_grammar = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_to_make_it_as_grammatically_simple_as_possible:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"simple_grammar\", \"sents\": simple_grammar})\n",
    "        middle_grammar = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_with_a_careful_focus_on_grammar.-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"middle_grammar\", \"sents\": middle_grammar})\n",
    "        difficult_grammar = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_to_make_it_as_grammatically_complex_as_possible:-.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"difficult_grammar\", \"sents\": difficult_grammar})\n",
    "        human_like_1 = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence_in_a_human-like_manner._It_has_been_reported_that_human_abili.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"human_like_1\", \"sents\": human_like_1})\n",
    "        human_like_2 = json.load(open(f\"../sampled/DC/{dir}/Please_complete_the_following_sentence._We_are_trying_to_reproduce_human_reading_times_with_the_word.json\"))\n",
    "        row_list.append({\"model\": dir, \"prompt\": \"human_like_2\", \"sents\": human_like_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(row_list)\n",
    "df[\"dep_len\"] = df[\"sents\"].apply(lambda x: mean(get_dep_lens(x)))\n",
    "df[\"sent_len\"] = df[\"sents\"].apply(lambda x: mean(get_sent_lens(x)))\n",
    "df[\"word_freq\"] = df[\"sents\"].apply(lambda x: mean(get_word_freqs(x, exclude_stopwords=True)))\n",
    "df[\"word_len\"] = df[\"sents\"].apply(lambda x: mean(get_word_lens(x)))\n",
    "df[\"id\"] = df[\"prompt\"].apply(lambda x: prompt2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompt2id.keys():\n",
    "    print(prompt2id[prompt], end=\" \")\n",
    "    for metric in [\"dep_len\", \"sent_len\", \"word_freq\", \"word_len\"]:\n",
    "        for model in model_orders:\n",
    "            print(\"& \", end=\"\")\n",
    "            if metric == \"sent_len\":\n",
    "                score = df[(df[\"prompt\"]==prompt) & (df[\"model\"]==model)][metric].apply(lambda x: '{:,.1f}'.format(x)).to_list()[0]\n",
    "            else:\n",
    "                score = df[(df[\"prompt\"]==prompt) & (df[\"model\"]==model)][metric].apply(lambda x: '{:,.2f}'.format(x)).to_list()[0]\n",
    "            print(score, end=\" \")\n",
    "    print(\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"../sampled/DC/Llama-2-70b-chat-hf/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    # print(file)\n",
    "    data = json.load(open(file))\n",
    "    output = data[7]\n",
    "    if \"Answer:\\n\" in output:\n",
    "        output = \"\\n\".join([line for line in output.split(\"\\n\") if line])\n",
    "        prompt = output.split(\"Answer:\\n\")[0] + \"Answer:\\n\" + \" \".join(output.split(\"Answer:\\n\")[1].split(\"\\n\")[0].split()[:5])\n",
    "        output = prompt +  \" \\\\textcolor{red}{\" + \" \".join(output.split(\"Answer:\\n\")[1].split()[5:]) + \"}\\n\"\n",
    "        output = output.replace(\"\\n\", \"\\\\\\\\\\n\").strip(\"\\n\")\n",
    "        print(output)\n",
    "        print(\"\\midrule\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
